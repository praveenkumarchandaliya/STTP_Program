{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "REVqkscd48O-",
        "outputId": "1ac56d31-2552-4f75-904a-f0e5f9b674ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install torchvision\n",
        "!  pip install torchsummary\n",
        "! pip install torch\n",
        "!pip install Pillow==5.3.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.0)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Collecting Pillow==5.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 10.9MB/s \n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.4.1\n",
            "    Uninstalling Pillow-5.4.1:\n",
            "      Successfully uninstalled Pillow-5.4.1\n",
            "Successfully installed Pillow-5.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "lhk6J5EYKDn-",
        "outputId": "64969130-8969-44ba-e55b-71ad1da81f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_DIR=\"drive/My Drive/iscd/train\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "czhy9dVv0b0p",
        "outputId": "461f481b-5948-47d9-dcad-c0507eb1519c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "import sys\n",
        "sys.argv=['--dataset','clf','--dataroot','drive/My Drive/iscd/train','--worker','2','--batchSize','32','--imageSize','64',\n",
        "          '--nz','100','--ngf','64','--ndf','64','--niter','11','--lr','.0002','--beta1','0.5','--ngpu','1'\n",
        "         ]\n",
        "parser  = argparse.ArgumentParser()\n",
        "parser.add_argument(\"input_file\",help=\"Input image, directory, or npy.\")\n",
        "parser.add_argument('--dataset',required=False,default='clf',help='clf')\n",
        "parser.add_argument('--dataroot',required=False,help='path to dataset')\n",
        "parser.add_argument('--worker',type=int,help='number of dataloading worker',default=2)\n",
        "parser.add_argument('--batchSize',type=int,default=32,help='input batch size')\n",
        "parser.add_argument('--imageSize',type=int,default=64,help='Size of the latent z vector')\n",
        "parser.add_argument('--nz',type=int,default=100)\n",
        "parser.add_argument('--ngf',type=int,default=64)\n",
        "parser.add_argument('--ndf',type=int,default=64)\n",
        "parser.add_argument('--niter',type=int,default=11,help='')\n",
        "parser.add_argument('--lr',type=float,default=0.0002,help='learning rate defult=0.0002')\n",
        "parser.add_argument('--beta1',type=float,default=0.5,help='number of gups to use')\n",
        "parser.add_argument('--cuda',action='store_true',help='enables cuda')\n",
        "parser.add_argument('--ngpu',type=int,default=1,help='enable cuda')\n",
        "parser.add_argument('--netG',default='',help='path to netG')\n",
        "parser.add_argument('--netD',default='',help='path to netD')\n",
        "parser.add_argument('--outf',default='.',help='folder to output image and model check point')\n",
        "parser.add_argument('--manualSeed',type=int,help='manual seed')\n",
        "\n",
        "args = parser.parse_args()\n",
        "print(args)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batchSize=32, beta1=0.5, cuda=False, dataroot='drive/My Drive/iscd/train', dataset='clf', imageSize=64, input_file='clf', lr=0.0002, manualSeed=None, ndf=64, netD='', netG='', ngf=64, ngpu=1, niter=11, nz=100, outf='.', worker=2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a7YETA_MBy8l",
        "outputId": "3c3552ac-0c35-4b95-83f8-10bf0fb32aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  os.makedirs(args.outf)\n",
        "except OSError:\n",
        "  pass\n",
        "if args.manualSeed is None:\n",
        "  args.manualSeed = random.randint(1,10000)\n",
        "print(\"Random Seed :\",args.manualSeed)\n",
        "random.seed(args.manualSeed)\n",
        "torch.manual_seed(args.manualSeed)\n",
        "if args.cuda:\n",
        "  torch.cuda.manual_seed_all(args.manualSeed)\n",
        "cudnn.benchmark=True\n",
        "if torch.cuda.is_available() and not args.cuda:\n",
        "  print(\"Warning You have a CUDA device so should probably run with --cuda\")\n",
        "if args.dataset in ['clf']:\n",
        "    # Foldr dataset\n",
        "    dataset = dset.ImageFolder(root=args.dataroot,\n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.Resize(args.imageSize),\n",
        "                                   transforms.CenterCrop(args.imageSize),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                               ]))\n",
        "dataloader = torch.utils.data.DataLoader(dataset,batch_size=args.batchSize,\n",
        "                                         shuffle=True,num_workers=int(args.worker))\n",
        "ngpu = int(args.ngpu)\n",
        "nz  = int(args.nz)\n",
        "ngf  = int(args.ngf)\n",
        "ndf  = int(args.ndf)\n",
        "nc  = 3\n",
        "def weight_init(m):\n",
        "    classname=m.__class__.__name__\n",
        "    if classname.find('Conv')!=-1:\n",
        "      m.weight.data.normal_(0.0,0.02)\n",
        "    elif classname.find('BatchNorm')!=-1:\n",
        "      m.weight.data.normal_(1.0,0.02)\n",
        "      m.bias.data.fill_(0)\n",
        "class _netG(nn.Module):\n",
        "  def __init__(self,ngpu):\n",
        "    super(_netG,self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.main = nn.Sequential(\n",
        "        #input is z going into a convoltuion into a convolution\n",
        "        nn.ConvTranspose2d(nz,ngf*8,4,1,0,bias=False),\n",
        "        nn.BatchNorm2d(ngf*8),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(ngf*8,ngf*4,4,2,1,bias=False),\n",
        "        nn.BatchNorm2d(ngf*4),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(ngf*4,ngf*2,4,2,1,bias=False),\n",
        "        nn.BatchNorm2d(ngf*2),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(ngf*2,ngf,4,2,1,bias=False),\n",
        "        nn.BatchNorm2d(ngf),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(ngf,nc,4,2,1,bias=False),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  def forward(self,input):\n",
        "      output  =self.main(input)\n",
        "      return output\n",
        "netG = _netG(ngpu)\n",
        "netG.apply(weight_init)\n",
        "if(args.netG!=''):\n",
        "  netG.load_state_dict(torch.load(args.netG))\n",
        "print(netG)\n",
        "class _netD(nn.Module):\n",
        "  def __init__(self,ngpu):\n",
        "    super(_netD,self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.main = nn.Sequential(\n",
        "         nn.Conv2d(nc,ndf,4,2,1,bias=False),\n",
        "         nn.LeakyReLU(0.2,inplace=True),\n",
        "         nn.Conv2d(ndf,ndf*2,4,2,1,bias=False),\n",
        "         nn.BatchNorm2d(ndf*2),\n",
        "         nn.LeakyReLU(0.2,inplace=True),\n",
        "         nn.Conv2d(ndf*2,ndf*4,4,2,1,bias=False),\n",
        "         nn.BatchNorm2d(ndf*4),\n",
        "         nn.LeakyReLU(0.2,inplace=True),\n",
        "         nn.Conv2d(ndf*4,ndf*8,4,2,1,bias=False),\n",
        "         nn.BatchNorm2d(ndf*8),\n",
        "         nn.LeakyReLU(0.2,inplace=True),\n",
        "         nn.Conv2d(ndf*8,1,4,1,0,bias=False),\n",
        "         nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self,input):\n",
        "      output  =self.main(input)\n",
        "      return output.view(-1,1).squeeze(1)\n",
        "netD = _netD(ngpu)\n",
        "netD.apply(weight_init)\n",
        "if args.netD!='':\n",
        "  netD.load_state_dict(troch.load(args.netD))\n",
        "print(netD)\n",
        "\n",
        "os.listdir('drive/My Drive/iscd/train')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed : 4201\n",
            "_netG(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n",
            "_netD(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['00']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "y1P79YOm47cQ",
        "outputId": "c5ef3640-c427-4bf4-a230-c730dfb35998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4822
        }
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "input = torch.FloatTensor(args.batchSize, 3, args.imageSize, args.imageSize)\n",
        "noise = torch.FloatTensor(args.batchSize, nz, 1, 1)\n",
        "fixed_noise = torch.FloatTensor(args.batchSize, nz, 1, 1).normal_(0, 1)\n",
        "label = torch.FloatTensor(args.batchSize)\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "if args.cuda:\n",
        "    netD.cuda()\n",
        "    netG.cuda()\n",
        "    criterion.cuda()\n",
        "    input, label = input.cuda(), label.cuda()\n",
        "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
        "\n",
        "fixed_noise = Variable(fixed_noise)\n",
        "\n",
        "# setup optimizer\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
        "\n",
        "for epoch in range(args.niter):\n",
        "    for i,data in enumerate(dataloader):\n",
        "        print(\"i\",i)\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        # train with real\n",
        "        netD.zero_grad()\n",
        "        real_cpu, _ = data\n",
        "        batch_size = real_cpu.size(0)\n",
        "        if args.cuda:\n",
        "            real_cpu = real_cpu.cuda()\n",
        "        input.resize_as_(real_cpu).copy_(real_cpu)\n",
        "        label.resize_(batch_size).fill_(real_label)\n",
        "        inputv = Variable(input)\n",
        "        labelv = Variable(label)\n",
        "\n",
        "        output = netD(inputv)\n",
        "        errD_real = criterion(output, labelv)\n",
        "        errD_real.backward()\n",
        "        D_x = output.data.mean()\n",
        "\n",
        "        # train with fake\n",
        "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
        "        noisev = Variable(noise)\n",
        "        fake = netG(noisev)\n",
        "        labelv = Variable(label.fill_(fake_label))\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, labelv)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.data.mean()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        labelv = Variable(label.fill_(real_label))  # fake labels are real for generator cost\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, labelv)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.data.mean()\n",
        "        optimizerG.step()\n",
        "\n",
        "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "              % (epoch, args.niter, i, len(dataloader),\n",
        "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        if i % 10 == 0:\n",
        "            vutils.save_image(real_cpu,\n",
        "                    '%s/real_samples.png' % args.outf,\n",
        "                    normalize=True)\n",
        "            fake = netG(fixed_noise)\n",
        "            vutils.save_image(fake.data,\n",
        "                    '%s/fake_samples_epoch_%03d.png' % (args.outf, epoch),\n",
        "                    normalize=True)\n",
        "\n",
        "    # do checkpointing\n",
        "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (args.outf, epoch))\n",
        "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (args.outf, epoch))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i 0\n",
            "[0/11][0/13] Loss_D: 2.8755 Loss_G: 1.8466 D(x): 0.1210 D(G(z)): 0.3381 / 0.1758\n",
            "i 1\n",
            "[0/11][1/13] Loss_D: 2.3269 Loss_G: 3.9216 D(x): 0.9998 D(G(z)): 0.8496 / 0.0301\n",
            "i 2\n",
            "[0/11][2/13] Loss_D: 0.7363 Loss_G: 5.8397 D(x): 1.0000 D(G(z)): 0.4596 / 0.0039\n",
            "i 3\n",
            "[0/11][3/13] Loss_D: 0.2091 Loss_G: 6.0256 D(x): 0.9999 D(G(z)): 0.1790 / 0.0032\n",
            "i 4\n",
            "[0/11][4/13] Loss_D: 0.1365 Loss_G: 5.5443 D(x): 0.9995 D(G(z)): 0.1206 / 0.0051\n",
            "i 5\n",
            "[0/11][5/13] Loss_D: 0.2743 Loss_G: 5.8510 D(x): 0.9853 D(G(z)): 0.1996 / 0.0034\n",
            "i 6\n",
            "[0/11][6/13] Loss_D: 0.1713 Loss_G: 6.2285 D(x): 0.9973 D(G(z)): 0.1422 / 0.0027\n",
            "i 7\n",
            "[0/11][7/13] Loss_D: 0.1719 Loss_G: 6.4419 D(x): 0.9925 D(G(z)): 0.1333 / 0.0020\n",
            "i 8\n",
            "[0/11][8/13] Loss_D: 0.0917 Loss_G: 6.4129 D(x): 0.9974 D(G(z)): 0.0813 / 0.0021\n",
            "i 9\n",
            "[0/11][9/13] Loss_D: 0.2166 Loss_G: 6.8835 D(x): 0.9710 D(G(z)): 0.1503 / 0.0014\n",
            "i 10\n",
            "[0/11][10/13] Loss_D: 0.1168 Loss_G: 6.7729 D(x): 0.9773 D(G(z)): 0.0759 / 0.0017\n",
            "i 11\n",
            "[0/11][11/13] Loss_D: 0.0674 Loss_G: 6.5762 D(x): 0.9907 D(G(z)): 0.0539 / 0.0020\n",
            "i 12\n",
            "[0/11][12/13] Loss_D: 0.1497 Loss_G: 7.5801 D(x): 0.9957 D(G(z)): 0.1293 / 0.0007\n",
            "i 0\n",
            "[1/11][0/13] Loss_D: 0.0490 Loss_G: 7.4427 D(x): 0.9879 D(G(z)): 0.0347 / 0.0009\n",
            "i 1\n",
            "[1/11][1/13] Loss_D: 0.0689 Loss_G: 6.5727 D(x): 0.9772 D(G(z)): 0.0397 / 0.0018\n",
            "i 2\n",
            "[1/11][2/13] Loss_D: 0.1728 Loss_G: 8.8569 D(x): 0.9969 D(G(z)): 0.1433 / 0.0002\n",
            "i 3\n",
            "[1/11][3/13] Loss_D: 0.0609 Loss_G: 8.2898 D(x): 0.9597 D(G(z)): 0.0102 / 0.0004\n",
            "i 4\n",
            "[1/11][4/13] Loss_D: 0.0748 Loss_G: 6.6062 D(x): 0.9690 D(G(z)): 0.0214 / 0.0021\n",
            "i 5\n",
            "[1/11][5/13] Loss_D: 0.1118 Loss_G: 7.8951 D(x): 0.9897 D(G(z)): 0.0897 / 0.0005\n",
            "i 6\n",
            "[1/11][6/13] Loss_D: 0.1904 Loss_G: 7.2688 D(x): 0.9575 D(G(z)): 0.0290 / 0.0009\n",
            "i 7\n",
            "[1/11][7/13] Loss_D: 0.0970 Loss_G: 7.5841 D(x): 0.9695 D(G(z)): 0.0556 / 0.0007\n",
            "i 8\n",
            "[1/11][8/13] Loss_D: 0.0387 Loss_G: 7.5882 D(x): 0.9952 D(G(z)): 0.0328 / 0.0007\n",
            "i 9\n",
            "[1/11][9/13] Loss_D: 0.0489 Loss_G: 8.1029 D(x): 0.9982 D(G(z)): 0.0454 / 0.0004\n",
            "i 10\n",
            "[1/11][10/13] Loss_D: 0.1171 Loss_G: 7.8022 D(x): 0.9516 D(G(z)): 0.0410 / 0.0006\n",
            "i 11\n",
            "[1/11][11/13] Loss_D: 0.0715 Loss_G: 9.1054 D(x): 0.9913 D(G(z)): 0.0577 / 0.0002\n",
            "i 12\n",
            "[1/11][12/13] Loss_D: 0.0376 Loss_G: 8.4014 D(x): 0.9864 D(G(z)): 0.0207 / 0.0004\n",
            "i 0\n",
            "[2/11][0/13] Loss_D: 0.0463 Loss_G: 8.6791 D(x): 0.9993 D(G(z)): 0.0434 / 0.0003\n",
            "i 1\n",
            "[2/11][1/13] Loss_D: 0.0473 Loss_G: 9.2327 D(x): 0.9976 D(G(z)): 0.0427 / 0.0001\n",
            "i 2\n",
            "[2/11][2/13] Loss_D: 0.0498 Loss_G: 8.6962 D(x): 0.9790 D(G(z)): 0.0262 / 0.0002\n",
            "i 3\n",
            "[2/11][3/13] Loss_D: 0.0707 Loss_G: 9.8303 D(x): 0.9865 D(G(z)): 0.0540 / 0.0001\n",
            "i 4\n",
            "[2/11][4/13] Loss_D: 0.0363 Loss_G: 8.5335 D(x): 0.9729 D(G(z)): 0.0062 / 0.0002\n",
            "i 5\n",
            "[2/11][5/13] Loss_D: 0.0625 Loss_G: 9.6763 D(x): 0.9948 D(G(z)): 0.0547 / 0.0001\n",
            "i 6\n",
            "[2/11][6/13] Loss_D: 0.0208 Loss_G: 8.8482 D(x): 0.9904 D(G(z)): 0.0108 / 0.0002\n",
            "i 7\n",
            "[2/11][7/13] Loss_D: 0.0916 Loss_G: 8.0294 D(x): 0.9519 D(G(z)): 0.0314 / 0.0004\n",
            "i 8\n",
            "[2/11][8/13] Loss_D: 0.0893 Loss_G: 12.5064 D(x): 0.9945 D(G(z)): 0.0763 / 0.0000\n",
            "i 9\n",
            "[2/11][9/13] Loss_D: 0.0199 Loss_G: 11.7235 D(x): 0.9821 D(G(z)): 0.0007 / 0.0000\n",
            "i 10\n",
            "[2/11][10/13] Loss_D: 0.0095 Loss_G: 8.6494 D(x): 0.9930 D(G(z)): 0.0021 / 0.0003\n",
            "i 11\n",
            "[2/11][11/13] Loss_D: 0.0746 Loss_G: 11.0001 D(x): 0.9973 D(G(z)): 0.0664 / 0.0000\n",
            "i 12\n",
            "[2/11][12/13] Loss_D: 0.0095 Loss_G: 9.9143 D(x): 0.9960 D(G(z)): 0.0053 / 0.0001\n",
            "i 0\n",
            "[3/11][0/13] Loss_D: 0.0230 Loss_G: 8.0523 D(x): 0.9896 D(G(z)): 0.0121 / 0.0005\n",
            "i 1\n",
            "[3/11][1/13] Loss_D: 0.1259 Loss_G: 15.7815 D(x): 0.9940 D(G(z)): 0.1097 / 0.0000\n",
            "i 2\n",
            "[3/11][2/13] Loss_D: 0.0276 Loss_G: 16.2018 D(x): 0.9740 D(G(z)): 0.0000 / 0.0000\n",
            "i 3\n",
            "[3/11][3/13] Loss_D: 0.0130 Loss_G: 13.1100 D(x): 0.9882 D(G(z)): 0.0000 / 0.0000\n",
            "i 4\n",
            "[3/11][4/13] Loss_D: 0.0734 Loss_G: 7.1492 D(x): 0.9464 D(G(z)): 0.0014 / 0.0010\n",
            "i 5\n",
            "[3/11][5/13] Loss_D: 0.2134 Loss_G: 20.6518 D(x): 0.9993 D(G(z)): 0.1835 / 0.0000\n",
            "i 6\n",
            "[3/11][6/13] Loss_D: 0.0098 Loss_G: 23.4552 D(x): 0.9906 D(G(z)): 0.0000 / 0.0000\n",
            "i 7\n",
            "[3/11][7/13] Loss_D: 0.0227 Loss_G: 21.6209 D(x): 0.9811 D(G(z)): 0.0000 / 0.0000\n",
            "i 8\n",
            "[3/11][8/13] Loss_D: 0.0053 Loss_G: 17.7066 D(x): 0.9948 D(G(z)): 0.0000 / 0.0000\n",
            "i 9\n",
            "[3/11][9/13] Loss_D: 0.0010 Loss_G: 12.4388 D(x): 0.9990 D(G(z)): 0.0000 / 0.0000\n",
            "i 10\n",
            "[3/11][10/13] Loss_D: 0.0054 Loss_G: 7.0651 D(x): 0.9963 D(G(z)): 0.0017 / 0.0012\n",
            "i 11\n",
            "[3/11][11/13] Loss_D: 0.1252 Loss_G: 15.8729 D(x): 0.9955 D(G(z)): 0.1109 / 0.0000\n",
            "i 12\n",
            "[3/11][12/13] Loss_D: 0.0070 Loss_G: 17.1986 D(x): 0.9932 D(G(z)): 0.0000 / 0.0000\n",
            "i 0\n",
            "[4/11][0/13] Loss_D: 0.0192 Loss_G: 14.9775 D(x): 0.9815 D(G(z)): 0.0000 / 0.0000\n",
            "i 1\n",
            "[4/11][1/13] Loss_D: 0.0085 Loss_G: 11.2989 D(x): 0.9918 D(G(z)): 0.0000 / 0.0000\n",
            "i 2\n",
            "[4/11][2/13] Loss_D: 0.0108 Loss_G: 6.9139 D(x): 0.9914 D(G(z)): 0.0018 / 0.0012\n",
            "i 3\n",
            "[4/11][3/13] Loss_D: 0.1322 Loss_G: 16.1841 D(x): 0.9888 D(G(z)): 0.1099 / 0.0000\n",
            "i 4\n",
            "[4/11][4/13] Loss_D: 0.0307 Loss_G: 17.1479 D(x): 0.9715 D(G(z)): 0.0000 / 0.0000\n",
            "i 5\n",
            "[4/11][5/13] Loss_D: 0.0067 Loss_G: 15.0844 D(x): 0.9935 D(G(z)): 0.0000 / 0.0000\n",
            "i 6\n",
            "[4/11][6/13] Loss_D: 0.0022 Loss_G: 11.0180 D(x): 0.9978 D(G(z)): 0.0001 / 0.0000\n",
            "i 7\n",
            "[4/11][7/13] Loss_D: 0.0230 Loss_G: 6.4745 D(x): 0.9823 D(G(z)): 0.0035 / 0.0021\n",
            "i 8\n",
            "[4/11][8/13] Loss_D: 0.2810 Loss_G: 25.8915 D(x): 0.9965 D(G(z)): 0.2332 / 0.0000\n",
            "i 9\n",
            "[4/11][9/13] Loss_D: 0.1974 Loss_G: 28.8905 D(x): 0.8630 D(G(z)): 0.0000 / 0.0000\n",
            "i 10\n",
            "[4/11][10/13] Loss_D: 0.0009 Loss_G: 29.2420 D(x): 0.9991 D(G(z)): 0.0000 / 0.0000\n",
            "i 11\n",
            "[4/11][11/13] Loss_D: 0.0006 Loss_G: 28.7487 D(x): 0.9994 D(G(z)): 0.0000 / 0.0000\n",
            "i 12\n",
            "[4/11][12/13] Loss_D: 0.0002 Loss_G: 27.1057 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
            "i 0\n",
            "[5/11][0/13] Loss_D: 0.0000 Loss_G: 25.0442 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
            "i 1\n",
            "[5/11][1/13] Loss_D: 0.0019 Loss_G: 20.4858 D(x): 0.9982 D(G(z)): 0.0000 / 0.0000\n",
            "i 2\n",
            "[5/11][2/13] Loss_D: 0.0025 Loss_G: 15.0204 D(x): 0.9976 D(G(z)): 0.0000 / 0.0000\n",
            "i 3\n",
            "[5/11][3/13] Loss_D: 0.0006 Loss_G: 9.4125 D(x): 0.9995 D(G(z)): 0.0001 / 0.0001\n",
            "i 4\n",
            "[5/11][4/13] Loss_D: 0.0325 Loss_G: 7.9063 D(x): 0.9997 D(G(z)): 0.0313 / 0.0005\n",
            "i 5\n",
            "[5/11][5/13] Loss_D: 0.0577 Loss_G: 12.6006 D(x): 0.9996 D(G(z)): 0.0552 / 0.0000\n",
            "i 6\n",
            "[5/11][6/13] Loss_D: 0.0133 Loss_G: 12.4084 D(x): 0.9891 D(G(z)): 0.0005 / 0.0000\n",
            "i 7\n",
            "[5/11][7/13] Loss_D: 0.0012 Loss_G: 9.8565 D(x): 0.9994 D(G(z)): 0.0006 / 0.0001\n",
            "i 8\n",
            "[5/11][8/13] Loss_D: 0.0111 Loss_G: 7.3731 D(x): 0.9981 D(G(z)): 0.0091 / 0.0009\n",
            "i 9\n",
            "[5/11][9/13] Loss_D: 0.1955 Loss_G: 26.0985 D(x): 0.9998 D(G(z)): 0.1719 / 0.0000\n",
            "i 10\n",
            "[5/11][10/13] Loss_D: 0.1741 Loss_G: 27.4404 D(x): 0.8641 D(G(z)): 0.0000 / 0.0000\n",
            "i 11\n",
            "[5/11][11/13] Loss_D: 0.0073 Loss_G: 26.1591 D(x): 0.9933 D(G(z)): 0.0000 / 0.0000\n",
            "i 12\n",
            "[5/11][12/13] Loss_D: 0.0011 Loss_G: 22.2992 D(x): 0.9989 D(G(z)): 0.0000 / 0.0000\n",
            "i 0\n",
            "[6/11][0/13] Loss_D: 0.0017 Loss_G: 16.0647 D(x): 0.9984 D(G(z)): 0.0000 / 0.0000\n",
            "i 1\n",
            "[6/11][1/13] Loss_D: 0.0020 Loss_G: 9.1486 D(x): 0.9982 D(G(z)): 0.0002 / 0.0002\n",
            "i 2\n",
            "[6/11][2/13] Loss_D: 0.0756 Loss_G: 13.9743 D(x): 0.9998 D(G(z)): 0.0704 / 0.0000\n",
            "i 3\n",
            "[6/11][3/13] Loss_D: 0.0356 Loss_G: 12.5589 D(x): 0.9771 D(G(z)): 0.0005 / 0.0000\n",
            "i 4\n",
            "[6/11][4/13] Loss_D: 0.0191 Loss_G: 9.4172 D(x): 0.9858 D(G(z)): 0.0008 / 0.0001\n",
            "i 5\n",
            "[6/11][5/13] Loss_D: 0.0726 Loss_G: 9.1388 D(x): 0.9763 D(G(z)): 0.0316 / 0.0001\n",
            "i 6\n",
            "[6/11][6/13] Loss_D: 0.0377 Loss_G: 12.2298 D(x): 0.9991 D(G(z)): 0.0355 / 0.0000\n",
            "i 7\n",
            "[6/11][7/13] Loss_D: 0.0059 Loss_G: 10.6776 D(x): 0.9968 D(G(z)): 0.0025 / 0.0000\n",
            "i 8\n",
            "[6/11][8/13] Loss_D: 0.0864 Loss_G: 6.3656 D(x): 0.9531 D(G(z)): 0.0079 / 0.0026\n",
            "i 9\n",
            "[6/11][9/13] Loss_D: 0.6483 Loss_G: 37.7605 D(x): 0.9996 D(G(z)): 0.4199 / 0.0000\n",
            "i 10\n",
            "[6/11][10/13] Loss_D: 6.1222 Loss_G: 30.7731 D(x): 0.0648 D(G(z)): 0.0000 / 0.0000\n",
            "i 11\n",
            "[6/11][11/13] Loss_D: 0.0006 Loss_G: 25.4226 D(x): 0.9994 D(G(z)): 0.0000 / 0.0000\n",
            "i 12\n",
            "[6/11][12/13] Loss_D: 0.0081 Loss_G: 14.3454 D(x): 0.9927 D(G(z)): 0.0000 / 0.0000\n",
            "i 0\n",
            "[7/11][0/13] Loss_D: 0.3226 Loss_G: 24.6452 D(x): 1.0000 D(G(z)): 0.2299 / 0.0000\n",
            "i 1\n",
            "[7/11][1/13] Loss_D: 0.2743 Loss_G: 23.9117 D(x): 0.9230 D(G(z)): 0.0000 / 0.0000\n",
            "i 2\n",
            "[7/11][2/13] Loss_D: 0.0461 Loss_G: 17.8280 D(x): 0.9709 D(G(z)): 0.0000 / 0.0000\n",
            "i 3\n",
            "[7/11][3/13] Loss_D: 0.0876 Loss_G: 9.0834 D(x): 0.9681 D(G(z)): 0.0001 / 0.0004\n",
            "i 4\n",
            "[7/11][4/13] Loss_D: 1.2036 Loss_G: 33.6691 D(x): 0.9887 D(G(z)): 0.5063 / 0.0000\n",
            "i 5\n",
            "[7/11][5/13] Loss_D: 5.8832 Loss_G: 23.7817 D(x): 0.0578 D(G(z)): 0.0000 / 0.0000\n",
            "i 6\n",
            "[7/11][6/13] Loss_D: 0.0000 Loss_G: 14.3215 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
            "i 7\n",
            "[7/11][7/13] Loss_D: 0.6819 Loss_G: 17.8247 D(x): 1.0000 D(G(z)): 0.2862 / 0.0000\n",
            "i 8\n",
            "[7/11][8/13] Loss_D: 0.0137 Loss_G: 16.4834 D(x): 1.0000 D(G(z)): 0.0127 / 0.0000\n",
            "i 9\n",
            "[7/11][9/13] Loss_D: 0.4502 Loss_G: 18.6620 D(x): 0.9864 D(G(z)): 0.1648 / 0.0000\n",
            "i 10\n",
            "[7/11][10/13] Loss_D: 0.0701 Loss_G: 16.4243 D(x): 0.9668 D(G(z)): 0.0042 / 0.0000\n",
            "i 11\n",
            "[7/11][11/13] Loss_D: 0.1532 Loss_G: 14.9821 D(x): 0.9879 D(G(z)): 0.0954 / 0.0000\n",
            "i 12\n",
            "[7/11][12/13] Loss_D: 0.1629 Loss_G: 14.3008 D(x): 0.9455 D(G(z)): 0.0627 / 0.0000\n",
            "i 0\n",
            "[8/11][0/13] Loss_D: 0.9176 Loss_G: 22.6302 D(x): 0.9110 D(G(z)): 0.2486 / 0.0000\n",
            "i 1\n",
            "[8/11][1/13] Loss_D: 0.6070 Loss_G: 16.9254 D(x): 0.7582 D(G(z)): 0.0003 / 0.0000\n",
            "i 2\n",
            "[8/11][2/13] Loss_D: 0.8550 Loss_G: 24.1318 D(x): 0.9802 D(G(z)): 0.2971 / 0.0000\n",
            "i 3\n",
            "[8/11][3/13] Loss_D: 0.0217 Loss_G: 22.4433 D(x): 0.9824 D(G(z)): 0.0000 / 0.0000\n",
            "i 4\n",
            "[8/11][4/13] Loss_D: 0.2602 Loss_G: 9.7411 D(x): 0.8922 D(G(z)): 0.0034 / 0.0006\n",
            "i 5\n",
            "[8/11][5/13] Loss_D: 4.7205 Loss_G: 32.9378 D(x): 0.9499 D(G(z)): 0.7614 / 0.0000\n",
            "i 6\n",
            "[8/11][6/13] Loss_D: 2.4732 Loss_G: 33.4425 D(x): 0.2966 D(G(z)): 0.0000 / 0.0000\n",
            "i 7\n",
            "[8/11][7/13] Loss_D: 0.0028 Loss_G: 33.5253 D(x): 0.9973 D(G(z)): 0.0000 / 0.0000\n",
            "i 8\n",
            "[8/11][8/13] Loss_D: 0.0312 Loss_G: 33.1849 D(x): 0.9756 D(G(z)): 0.0000 / 0.0000\n",
            "i 9\n",
            "[8/11][9/13] Loss_D: 0.0000 Loss_G: 32.8722 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
            "i 10\n",
            "[8/11][10/13] Loss_D: 0.0149 Loss_G: 32.8234 D(x): 0.9881 D(G(z)): 0.0000 / 0.0000\n",
            "i 11\n",
            "[8/11][11/13] Loss_D: 0.0000 Loss_G: 32.6257 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
            "i 12\n",
            "[8/11][12/13] Loss_D: 0.0000 Loss_G: 32.4146 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
            "i 0\n",
            "[9/11][0/13] Loss_D: 0.0005 Loss_G: 32.2043 D(x): 0.9995 D(G(z)): 0.0000 / 0.0000\n",
            "i 1\n",
            "[9/11][1/13] Loss_D: 0.0000 Loss_G: 31.7180 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
            "i 2\n",
            "[9/11][2/13] Loss_D: 0.0009 Loss_G: 31.1766 D(x): 0.9991 D(G(z)): 0.0000 / 0.0000\n",
            "i 3\n",
            "[9/11][3/13] Loss_D: 0.0001 Loss_G: 30.7400 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
            "i 4\n",
            "[9/11][4/13] Loss_D: 0.0000 Loss_G: 29.2367 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
            "i 5\n",
            "[9/11][5/13] Loss_D: 0.0027 Loss_G: 26.7157 D(x): 0.9974 D(G(z)): 0.0000 / 0.0000\n",
            "i 6\n",
            "[9/11][6/13] Loss_D: 0.0000 Loss_G: 17.7423 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
            "i 7\n",
            "[9/11][7/13] Loss_D: 1.1932 Loss_G: 24.4663 D(x): 0.9999 D(G(z)): 0.3965 / 0.0000\n",
            "i 8\n",
            "[9/11][8/13] Loss_D: 0.0038 Loss_G: 23.9288 D(x): 0.9963 D(G(z)): 0.0000 / 0.0000\n",
            "i 9\n",
            "[9/11][9/13] Loss_D: 0.4730 Loss_G: 15.8128 D(x): 0.9106 D(G(z)): 0.0000 / 0.0000\n",
            "i 10\n",
            "[9/11][10/13] Loss_D: 0.1358 Loss_G: 7.1117 D(x): 0.9525 D(G(z)): 0.0299 / 0.0032\n",
            "i 11\n",
            "[9/11][11/13] Loss_D: 2.7306 Loss_G: 30.3767 D(x): 0.9989 D(G(z)): 0.7317 / 0.0000\n",
            "i 12\n",
            "[9/11][12/13] Loss_D: 9.7838 Loss_G: 24.9389 D(x): 0.0115 D(G(z)): 0.0000 / 0.0000\n",
            "i 0\n",
            "[10/11][0/13] Loss_D: 0.0250 Loss_G: 15.6942 D(x): 0.9788 D(G(z)): 0.0000 / 0.0000\n",
            "i 1\n",
            "[10/11][1/13] Loss_D: 0.1059 Loss_G: 6.4704 D(x): 1.0000 D(G(z)): 0.0803 / 0.0173\n",
            "i 2\n",
            "[10/11][2/13] Loss_D: 3.0047 Loss_G: 20.3895 D(x): 0.9999 D(G(z)): 0.6407 / 0.0000\n",
            "i 3\n",
            "[10/11][3/13] Loss_D: 0.3887 Loss_G: 22.5213 D(x): 0.9079 D(G(z)): 0.0000 / 0.0000\n",
            "i 4\n",
            "[10/11][4/13] Loss_D: 1.6633 Loss_G: 10.1420 D(x): 0.6310 D(G(z)): 0.0000 / 0.0002\n",
            "i 5\n",
            "[10/11][5/13] Loss_D: 0.2185 Loss_G: 7.2453 D(x): 0.9891 D(G(z)): 0.1419 / 0.0032\n",
            "i 6\n",
            "[10/11][6/13] Loss_D: 0.8831 Loss_G: 17.5095 D(x): 0.9908 D(G(z)): 0.4382 / 0.0000\n",
            "i 7\n",
            "[10/11][7/13] Loss_D: 1.8496 Loss_G: 12.7690 D(x): 0.5839 D(G(z)): 0.0000 / 0.0000\n",
            "i 8\n",
            "[10/11][8/13] Loss_D: 0.3574 Loss_G: 4.8497 D(x): 0.8820 D(G(z)): 0.0109 / 0.0503\n",
            "i 9\n",
            "[10/11][9/13] Loss_D: 2.1084 Loss_G: 17.5915 D(x): 0.9543 D(G(z)): 0.6729 / 0.0000\n",
            "i 10\n",
            "[10/11][10/13] Loss_D: 1.5511 Loss_G: 13.2661 D(x): 0.4854 D(G(z)): 0.0000 / 0.0000\n",
            "i 11\n",
            "[10/11][11/13] Loss_D: 0.1341 Loss_G: 6.9197 D(x): 0.9064 D(G(z)): 0.0070 / 0.0146\n",
            "i 12\n",
            "[10/11][12/13] Loss_D: 1.4788 Loss_G: 16.4055 D(x): 0.9914 D(G(z)): 0.6269 / 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G4n2dQLUX16x"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}